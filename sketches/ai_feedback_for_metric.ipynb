{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_turbo = dspy.OpenAI(model='gpt-3.5-turbo-1106', max_tokens=300, temperature=1)\n",
    "dspy.settings.configure(lm=gpt3_turbo, max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the signature for automatic assessments.\n",
    "class Assess(dspy.Signature):\n",
    "    \"\"\"Assess the quality of a tweet along the specified dimension.\"\"\"\n",
    "\n",
    "    assessed_text = dspy.InputField()\n",
    "    assessment_question = dspy.InputField()\n",
    "    assessment_answer = dspy.OutputField(desc=\"Yes or No\")\n",
    "    \n",
    "def metric(gold, pred, trace=None):\n",
    "    question, answer, tweet = gold.question, gold.answer, pred.output\n",
    "\n",
    "    engaging = \"Does the assessed text make for a self-contained, engaging tweet?\"\n",
    "    correct = f\"The text should answer `{question}` with `{answer}`. Does the assessed text contain this answer?\"\n",
    "    \n",
    "    with dspy.context(lm=gpt3_turbo):\n",
    "        correct =  dspy.Predict(Assess)(assessed_text=tweet, assessment_question=correct)\n",
    "        engaging = dspy.Predict(Assess)(assessed_text=tweet, assessment_question=engaging)\n",
    "\n",
    "    correct, engaging = [m.assessment_answer.lower() == 'yes' for m in [correct, engaging]]\n",
    "    score = (correct + engaging) if correct and (len(tweet) <= 280) else 0\n",
    "\n",
    "    if trace is not None: return score >= 2\n",
    "    return score / 2.0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
